---
layout: single
author_profile: true
title: "Jiandong Ding (ä¸å»ºæ ‹)"
classes: wide
---

<style>
  .page__title { display: none; }
</style>

<style>
  /* --- æ ¸å¿ƒå¡ç‰‡å®¹å™¨ --- */
  .paper-card {
    display: flex;
    gap: 25px; /* å¢åŠ ä¸€ç‚¹å›¾æ–‡é—´è· */
    margin-bottom: 35px;
    padding-bottom: 25px;
    border-bottom: 1px solid #f0f0f0; /* é¢œè‰²æ›´æ·¡ä¸€ç‚¹ï¼Œæ˜¾é«˜çº§ */
    font-size: 0.95em; /* å­—ä½“ç¨å¾®ç¼©å°ä¸€ç‚¹ï¼Œæ˜¾å¾—æ›´ç²¾è‡´ */
    line-height: 1.6;  /* å¢åŠ è¡Œé«˜ï¼Œæå‡é˜…è¯»èˆ’é€‚åº¦ */
    color: #444;       /* å­—ä½“é¢œè‰²ç¨å¾®æŸ”å’Œä¸€ç‚¹ */
  }
  .paper-card:last-child { border-bottom: none; }
  
  /* --- å›¾ç‰‡åŒºåŸŸä¼˜åŒ– --- */
  .paper-img {
    flex: 0 0 35%; /* ã€ä¼˜åŒ–ç‚¹ã€‘å»ºè®®35%ï¼Œæ¯”40%æ›´åè°ƒï¼Œç•™ç»™æ–‡å­—æ›´å¤šç©ºé—´ */
    max-width: 380px; /* é™åˆ¶æœ€å¤§å®½åº¦ï¼Œé˜²æ­¢åœ¨å¤§å±ä¸Šå›¾å¤ªå¤§ */
#    display: none; /* é»˜è®¤éšè—ï¼Œæœ‰å›¾æ—¶åœ¨HTMLé‡ŒåŠ  style="display:block" */
  }
  
  .paper-img img {
    width: 100%;
    border-radius: 6px;
    border: 1px solid #e1e4e8; /* ç»™å›¾ç‰‡åŠ ä¸ªç»†å¾®çš„è¾¹æ¡†ï¼Œæ›´æœ‰è´¨æ„Ÿ */
    box-shadow: 0 4px 10px rgba(0,0,0,0.03); /* å¢åŠ ä¸€ç‚¹æ‚¬æµ®æ„Ÿ */
  }
  
  /* --- æ–‡å­—åŒºåŸŸ --- */
  .paper-content {
    flex: 1; 
  }
  
  /* è®ºæ–‡æ ‡é¢˜æ ·å¼ */
  .paper-title {
    font-size: 1.1em;
    font-weight: 700;
    color: #2c3e50;
    display: block;
    margin-bottom: 8px;
  }

  /* --- æ ‡ç­¾æ ·å¼ --- */
  .hl-tag {
    font-size: 0.8em;
    font-weight: bold;
    text-transform: uppercase;
    padding: 2px 6px;
    border-radius: 4px;
    margin-right: 6px;
    vertical-align: middle;
  }
  .tag-prob { background-color: #fff0ed; color: #c0392b; border: 1px solid #fadbd8; } 
  .tag-sol { background-color: #e8f8f5; color: #16a085; border: 1px solid #d1f2eb; } 
  .tag-method { background-color: #ebf5fb; color: #2980b9; border: 1px solid #d4e6f1; }
  
  /* --- æ‰‹æœºç«¯é€‚é… --- */
  @media (max-width: 768px) {
    .paper-card { flex-direction: column; gap: 15px; }
    .paper-img { flex: 0 0 100%; max-width: 100%; margin-bottom: 10px; }
  }
</style>

I am a **Principal Algorithm Expert** at **Huawei Technologies**, bridging the gap between theoretical algorithms and industrial-scale systems.

My research philosophy is summarized as **"From Biological Sequences to User Behaviors"**. I apply deep representation learning to decipher underlying patterns in dataâ€”from genomic sequences in my early career to billion-scale user behavior logs in commercial recommendation systems today.

Currently, I focus on building **Trustworthy & Scalable AI Systems**, with specific interests in:
* **Trustworthy RecSys:** Causal Inference, Debiasing, and Fairness.
* **System Efficiency:** Retrieval Architecture, Model Compression, and On-device AI.
* **LLM & Agents:** Large Language Models for BI and Data Agents.
* **Data-Centric AI:** Weak Supervision, Data Programming, and Automated Labeling.

---

## ğŸ”¥ News

* **[Jan 2026]** ğŸ‰ Paper *"Mitigating Popularity Bias in Recommendation"* accepted by **ACM TOIS**.
* **[Nov 2025]** ğŸš€ Two papers accepted! *"RPE4Rec"* by **WSDM 2026**, and *"Invariant Feature Learning"* by **AAAI 2026**.
* **[Dec 2024]** Paper *"BIS: NL2SQL Service Evaluation Benchmark"* accepted by **ICSOC 2024**.
* **[Aug 2024]** Paper on Low-rank Compression for CTR prediction accepted by **KDD 2024**.

---

## ğŸ¯ Research Highlights

My current research focuses on four core challenges: **Trustworthiness**, **Efficiency**, **LLM Agents**, and **Data Quality**.

### 1. Trustworthy & Unbiased Recommendation

<div class="paper-card">
  <div class="paper-img" style="display:block"> 
    <img src="/assets/images/TOIS2026.png" alt="TOIS Framework">
  </div>
  <div class="paper-content">
    <span class="paper-title">[TOIS 2026] Mitigating Popularity Bias with Global Listwise Learning</span>
    <span class="hl-tag tag-prob">Problem</span> Recommender systems often trap users in "echo chambers" or confuse popularity with genuine interest.
    <br><br>
    <span class="hl-tag tag-sol">Breakthrough</span> We proposed a <strong>Global Listwise Learning</strong> framework with progressive bi-weighting. Unlike pointwise methods, it optimizes the entire ranking list to balance long-tail exposure without sacrificing user satisfaction.
  </div>
</div>

<div class="paper-card">
  <div class="paper-img" style="display:block"> 
    <img src="/assets/images/AAAI2026.png" alt="AAAI Framework">
  </div>
  <div class="paper-content">
    <span class="paper-title">[AAAI 2026] Causal Inference for Watch-time Prediction</span>
    <span class="hl-tag tag-prob">Problem</span> In short-video feeds, "duration biases" (longer videos naturally get more watch time) mislead algorithms.
    <br><br>
    <span class="hl-tag tag-sol">Breakthrough</span> We introduced <strong>Invariant Feature Learning</strong> based on counterfactual inference to uncover the user's <em>true</em> willingness to watch, independent of video length.
  </div>
</div>

### 2. Extreme Efficiency at Scale

<div class="paper-card">
  <div class="paper-img" style="display:block"> 
    <img src="/assets/images/WSDM2026.png" alt="WSDM Framework">
  </div>
  <div class="paper-content">
    <span class="paper-title">[WSDM 2026] RPE4Rec: High-Efficiency Dynamic Retrieval</span>
    <span class="hl-tag tag-prob">Problem</span> Advanced Transformers are often too slow for real-time retrieval on billion-scale items.
    <br><br>
    <span class="hl-tag tag-sol">Breakthrough</span> We designed a novel <strong>Relative Position Encoding (RPE)</strong> mechanism specifically for dynamic node retrieval. This architecture significantly reduces inference latency while capturing complex sequential patterns.
  </div>
</div>

<div class="paper-card">
  <div class="paper-img" style="display:block"> 
    <img src="/assets/images/KDD2024.png" alt="KDD Framework">
  </div>
  <div class="paper-content">
    <span class="paper-title">[KDD 2024] Low-Rank Compression for CTR Prediction</span>
    <span class="hl-tag tag-sol">Breakthrough</span> A unified framework to compress massive CTR models using low-rank factorization, enabling high-performance ranking on resource-constrained devices (e.g., mobile phones).
  </div>
</div>

### 3. Large Models: Systems & Agents

<div class="paper-card">
  <div class="paper-img" style="display:block"> 
    <img src="/assets/images/ICSOC2025.png" alt="ICSOC Framework">
  </div>
  <div class="paper-content">
    <span class="paper-title">[ICSOC 2025] NL2SQL Benchmark for Business Intelligence</span>
    <span class="hl-tag tag-sol">Intelligence Layer</span> Evaluating how Large Language Models (LLMs) act as Data Agents to translate natural language into complex SQL queries, enabling automated business decision-making.
  </div>
</div>

<div class="paper-card">
  <div class="paper-img" style="display:block"> 
    <img src="/assets/images/PD2024.png" alt="PD Framework">
  </div>
  <div class="paper-content">
    <span class="paper-title">[arXiv 2024] P/D-Serve: Serving Disaggregated LLMs at Scale</span>
    <span class="hl-tag tag-sol">System Layer</span> We proposed a disaggregated serving architecture that optimizes KV-cache management and scheduling, significantly improving throughput for large-scale deployment.
  </div>
</div>

### 4. Data-Centric AI: Weak Supervision & Augmentation

<div class="paper-card">
  <div class="paper-img" style="display:block"> 
    <img src="/assets/images/NIPS2021.png" alt="DPSSL Framework">
  </div>
  <div class="paper-content">
    <span class="paper-title">[NeurIPS 2021] DP-SSL: Robust Semi-supervised Learning via Data Programming</span>
    <span class="hl-tag tag-prob">Problem</span> Deep learning performance heavily relies on massive labeled data, which is expensive and slow to obtain.
    <br><br>
    <span class="hl-tag tag-sol">Breakthrough</span> We introduced a <strong>Data Programming (DP)</strong> scheme into semi-supervised learning. It automatically generates probabilistic labels for unlabeled data, achieving SOTA performance even with only <strong>40 labeled samples</strong> per class.
  </div>
</div>

<div class="paper-card">
  <div class="paper-img" style="display:block"> 
    <img src="/assets/images/EMNLP2021.png" alt="SSP Framework">
  </div>
  <div class="paper-content">
    <span class="paper-title">[EMNLP 2021] Weakly-supervised Text Classification via Keyword Graph</span>
    <span class="hl-tag tag-method">Methodology</span> Instead of manual labeling, we utilize <strong>Keyword Graphs</strong> to propagate weak supervision signals. By modeling the correlation among keywords with GNNs, we can generate high-quality pseudo-labels for massive unlabeled corpora automatically.
  </div>
</div>
